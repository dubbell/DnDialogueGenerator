{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from data_collection import training_set\n",
    "from transformers import GPT2Tokenizer\n",
    "import pickle\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "romantic_data = load_dataset(\"AlekseyKorshuk/synthetic-romantic-characters\")[\"train\"]\n",
    "friendly_data = load_dataset(\"AlekseyKorshuk/synthetic-friendly-characters\")[\"train\"]\n",
    "fight_data = load_dataset(\"AlekseyKorshuk/synthetic-fight-characters\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "train_data = training_set([\"romantic\"], tokenizer)\n",
    "# print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'social justice,romance,inspiration,romantic<|endoftext|>compassionate,socially aware,passionate about human rights,always wears a bracelet with an equality symbol,quotes influential activists<|endoftext|>*Santiago walks up to you, wearing a shirt with an activist quote on it.* Hey there, I couldn’t help but notice you sitting here alone. Mind if I join you? *He smiles warmly and sits down.* I’m Santiago, by the way. What’s your name?\\n*smile back* Hi Santiago, nice to meet you. My name is [Your Name].\\n*Santiago nods in understanding* I see. Well, it’s a pleasure to meet you, [Your Name]. I’m an activist myself, fighting for social justice and equality. I believe that every person deserves the same opportunities in life, regardless of their race, gender, or background. It’s something that truly resonates with me, and I’m always eager to share my passion with others.\\n*nodding* That’s amazing, it’s great to see people fighting for a cause they believe in.\\n*Santiago smiles* Thank you, it truly is. And I believe that love and compassion are key to creating a better world. We need to embrace our differences and support each other, rather than tearing each other down. I think that if we all strive to be a little kinder and more understanding, we can make a real difference in the world.\\n*nodding* I completely agree. It starts with small steps and being kind to one another.\\n*Santiago leans in, his eyes sparkling with passion* You know, I was just thinking. Perhaps we could make a difference together. What do you say we go out and make a positive impact in the world, side by side? It could be an adventure, and who knows what kind of a difference we could make.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(train_data[1].unsqueeze(0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name', 'categories', 'personalities', 'description', 'conversation']\n"
     ]
    }
   ],
   "source": [
    "dialogue_dataset = []\n",
    "for dataset in [romantic_data, friendly_data, fight_data]:\n",
    "    for conversation in dataset:\n",
    "        dialogue_dataset.append(conversation)\n",
    "    \n",
    "print(list(dialogue_dataset[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, there are 17668 conversations, 1565 conversation categories, and 777 types of personalities.\n",
      "5 most common categories:\n",
      "\t3796\tromance\n",
      "\t2574\tfantasy\n",
      "\t1621\tadventure\n",
      "\t1578\tcomedy\n",
      "\t1337\tentertainment\n",
      "5 most common personalities:\n",
      "\t1582\tadventurous\n",
      "\t1211\tempathetic\n",
      "\t970\tmysterious\n",
      "\t956\tcalm\n",
      "\t805\tcreative\n",
      "\n",
      "In romantic_data, there are 5744 conversations, 582 conversation categories, and 244 types of personalities.\n",
      "5 most common categories:\n",
      "\t3645\tromance\n",
      "\t754\tfantasy\n",
      "\t752\ttravel\n",
      "\t740\tmusic\n",
      "\t591\tart\n",
      "5 most common personalities:\n",
      "\t774\tadventurous\n",
      "\t558\tcreative\n",
      "\t507\tcharming\n",
      "\t475\timaginative\n",
      "\t470\tcharismatic\n",
      "\n",
      "In friendly_data, there are 3871 conversations, 531 conversation categories, and 266 types of personalities.\n",
      "5 most common categories:\n",
      "\t689\tsupport\n",
      "\t671\tentertainment\n",
      "\t495\tcomedy\n",
      "\t440\teducation\n",
      "\t352\twellness\n",
      "5 most common personalities:\n",
      "\t682\tempathetic\n",
      "\t369\tcompassionate\n",
      "\t363\tadventurous\n",
      "\t359\tcalm\n",
      "\t339\tcurious\n",
      "\n",
      "In fight_data, there are 8053 conversations, 1169 conversation categories, and 634 types of personalities.\n",
      "5 most common categories:\n",
      "\t1785\tfantasy\n",
      "\t1266\tadventure\n",
      "\t1067\tcomedy\n",
      "\t958\tmystery\n",
      "\t609\taction\n",
      "5 most common personalities:\n",
      "\t789\tsarcastic\n",
      "\t716\tmysterious\n",
      "\t519\tcunning\n",
      "\t501\trebellious\n",
      "\t489\tmischievous\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_data(name, dataset):\n",
    "    category_counter = Counter()\n",
    "    personality_counter = Counter()\n",
    "    for conversation in dataset:\n",
    "        category_counter.update([c.lower() for c in conversation[\"categories\"]])\n",
    "        personality_counter.update([p.lower() for p in filter(lambda p: \" \" not in p, conversation[\"personalities\"])])\n",
    "\n",
    "    print(f\"In {name}, there are {len(dataset)} conversations, {len(category_counter)} conversation categories, and {len(personality_counter)} types of personalities.\")\n",
    "    print(\"5 most common categories:\")\n",
    "    for name, count in category_counter.most_common(5):\n",
    "        print(\"\\t\" + str(count) + \"\\t\" + str(name))\n",
    "    print(\"5 most common personalities:\")\n",
    "    for name, count in personality_counter.most_common(5):\n",
    "        print(\"\\t\" + str(count) + \"\\t\" + str(name))\n",
    "    print()\n",
    "    \n",
    "\n",
    "for name, dataset in zip([\"total\", \"romantic_data\", \"friendly_data\", \"fight_data\"], [dialogue_dataset, romantic_data, friendly_data, fight_data]):\n",
    "    count_data(name, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Personality statistics:\n",
      "4.364161195381481 0.591856041784797 2 10\n",
      "\n",
      "Category statistics:\n",
      "2.91283676703645 0.2858611286202068 2 4\n",
      "\n",
      "Conversation statistics:\n",
      "7 7\n",
      "10.0 3.1622776601683795 4 13\n",
      "50.0 0.0 50 50\n",
      "\n",
      "Total length statistics:\n",
      "1715.6083314466832 466.0260407307866 473 4622\n",
      "312.28656327824314 84.09137336614894 82 792\n"
     ]
    }
   ],
   "source": [
    "print(\"Personality statistics:\")\n",
    "personality_counts = [len(item[\"personalities\"]) for item in dialogue_dataset]\n",
    "print(np.mean(personality_counts), np.std(personality_counts), min(personality_counts), max(personality_counts))\n",
    "\n",
    "print(\"\\nCategory statistics:\")\n",
    "category_counts = [len(item[\"categories\"]) for item in dialogue_dataset]\n",
    "print(np.mean(category_counts), np.std(category_counts), min(category_counts), max(category_counts))\n",
    "\n",
    "print(\"\\nConversation statistics:\")\n",
    "line_counts = [len(item[\"conversation\"]) for item in dialogue_dataset]\n",
    "print(min(line_counts), max(line_counts))\n",
    "line_lengths = [len(line) for item in dialogue_dataset for line in item]\n",
    "print(np.mean(line_lengths), np.std(line_lengths), min(line_lengths), max(line_lengths))\n",
    "conversation_lengths = [sum(len(line) for line in item) for item in dialogue_dataset]\n",
    "print(np.mean(conversation_lengths), np.std(conversation_lengths), min(conversation_lengths), max(conversation_lengths))\n",
    "\n",
    "print(\"\\nTotal length statistics:\")\n",
    "char_lengths = []\n",
    "for item in dialogue_dataset:\n",
    "    temp = sum(len(line[\"content\"]) for line in item[\"conversation\"])\n",
    "    temp += sum(len(pers) for pers in item[\"personalities\"])\n",
    "    temp += sum(len(cat) for cat in item[\"categories\"])\n",
    "    char_lengths.append(temp)\n",
    "print(np.mean(char_lengths), np.std(char_lengths), min(char_lengths), max(char_lengths))\n",
    "\n",
    "token_lengths = []\n",
    "for item in dialogue_dataset:\n",
    "    temp = sum(len(line[\"content\"].split(\" \")) for line in item[\"conversation\"])\n",
    "    temp += sum(len(pers.split(\" \")) for pers in item[\"personalities\"])\n",
    "    temp += sum(len(cat.split(\" \")) for cat in item[\"categories\"])\n",
    "    token_lengths.append(temp)\n",
    "print(np.mean(token_lengths), np.std(token_lengths), min(token_lengths), max(token_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/linulun/.cache/huggingface/datasets/AlekseyKorshuk___parquet/AlekseyKorshuk--synthetic-romantic-characters-3b16d8e672467bfe/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f58b21c14249e0a84ade757d0ec309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/linulun/.cache/huggingface/datasets/AlekseyKorshuk___parquet/AlekseyKorshuk--synthetic-friendly-characters-8195740b6ede92c1/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca452ba05c1140a99ce26b43cd896160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/linulun/.cache/huggingface/datasets/AlekseyKorshuk___parquet/AlekseyKorshuk--synthetic-fight-characters-dbee9baf48903647/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a783be6e3d4b9c838990d1b48dc6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "test = training_set([\"romantic\", \"friendly\", \"fight\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(test, open(\"dataset.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<data_collection.DialogueDataset object at 0x728e22b6b290>\n"
     ]
    }
   ],
   "source": [
    "print(pickle.load(open(\"dataset.p\", \"rb\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.tensor([1]).squeeze().dim())\n",
    "print(torch.tensor([1, 1]).squeeze().dim())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
